#!/bin/bash

# =============================================================================
# DEPLOY SNOWFLAKE-BIGQUERY DEDUPLICATION SYNC
# =============================================================================
# This script deploys the deduplication sync Cloud Function

set -e

PROJECT_ID=$(gcloud config get-value project)
REGION="us-central1"
FUNCTION_NAME="snowflake-bq-deduplication-sync"
RUNTIME="python311"
MEMORY="1GB"
TIMEOUT="900s"  # 15 minutes for comprehensive sync
SERVICE_ACCOUNT="karbon-bq-sync@${PROJECT_ID}.iam.gserviceaccount.com"

echo "üöÄ Deploying Snowflake-BigQuery Deduplication Sync..."
echo "  Function: ${FUNCTION_NAME}"
echo "  Project: ${PROJECT_ID}"
echo "  Region: ${REGION}"
echo "  Service Account: ${SERVICE_ACCOUNT}"

# Create a temporary directory for the function
TEMP_DIR=$(mktemp -d)
echo "üìÅ Creating function package in: ${TEMP_DIR}"

# Copy the main script
cp snowflake_bq_deduplication_sync.py "${TEMP_DIR}/main.py"

# Create requirements.txt
cat > "${TEMP_DIR}/requirements.txt" << EOF
google-cloud-bigquery>=3.4.0
google-cloud-secret-manager>=2.16.0
snowflake-connector-python>=3.0.0
EOF

echo "üì¶ Function package created"

# Deploy the function
echo "üöÄ Deploying Cloud Function..."

gcloud functions deploy ${FUNCTION_NAME} \
  --gen2 \
  --runtime=${RUNTIME} \
  --region=${REGION} \
  --source=${TEMP_DIR} \
  --entry-point=deduplication_sync_cloud_function \
  --trigger-http \
  --memory=${MEMORY} \
  --timeout=${TIMEOUT} \
  --min-instances=0 \
  --max-instances=1 \
  --project=${PROJECT_ID} \
  --service-account=${SERVICE_ACCOUNT} \
  --allow-unauthenticated

echo "‚úÖ Cloud Function deployed successfully!"

# Clean up
rm -rf "${TEMP_DIR}"

# Create scheduler job for weekly deduplication
echo "‚è∞ Creating weekly scheduler job..."

SCHEDULER_JOB_NAME="snowflake-bq-deduplication-weekly"
FUNCTION_URL="https://${REGION}-${PROJECT_ID}.cloudfunctions.net/${FUNCTION_NAME}"

# Check if scheduler job already exists
if gcloud scheduler jobs describe ${SCHEDULER_JOB_NAME} --location=${REGION} &>/dev/null; then
    echo "‚ö†Ô∏è Scheduler job already exists, updating..."
    gcloud scheduler jobs update http ${SCHEDULER_JOB_NAME} \
        --location=${REGION} \
        --schedule="0 2 * * 0" \
        --uri="${FUNCTION_URL}" \
        --http-method=POST \
        --oidc-service-account-email="${SERVICE_ACCOUNT}" \
        --message-body='{"dry_run": false, "days_back": 30}'
else
    echo "üìÖ Creating new scheduler job..."
    gcloud scheduler jobs create http ${SCHEDULER_JOB_NAME} \
        --location=${REGION} \
        --schedule="0 2 * * 0" \
        --uri="${FUNCTION_URL}" \
        --http-method=POST \
        --oidc-service-account-email="${SERVICE_ACCOUNT}" \
        --message-body='{"dry_run": false, "days_back": 30}'
fi

echo ""
echo "üéâ DEDUPLICATION SYNC DEPLOYMENT COMPLETE!"
echo "==========================================="
echo ""
echo "‚úÖ Function URL: ${FUNCTION_URL}"
echo "‚úÖ Scheduler: Weekly on Sunday at 2:00 AM UTC"
echo "‚úÖ Service Account: ${SERVICE_ACCOUNT}"
echo ""
echo "üìä FEATURES:"
echo "   ‚Ä¢ Validates data consistency between Snowflake and BigQuery"
echo "   ‚Ä¢ Removes orphaned BigQuery records not found in Snowflake"
echo "   ‚Ä¢ Provides detailed reporting on cleanup actions"
echo "   ‚Ä¢ Handles all core tables automatically"
echo ""
echo "üîç USAGE:"
echo "   ‚Ä¢ Manual trigger (dry run): curl -X POST '${FUNCTION_URL}' -d '{\"dry_run\": true}'"
echo "   ‚Ä¢ Manual trigger (live): curl -X POST '${FUNCTION_URL}' -d '{\"dry_run\": false}'"
echo "   ‚Ä¢ Validate specific item: curl -X POST '${FUNCTION_URL}' -d '{\"work_item_id\": \"JvqmhFJBFGP\"}'"
echo "   ‚Ä¢ Check logs: gcloud logging read 'resource.type=cloud_function AND resource.labels.function_name=${FUNCTION_NAME}'"
echo ""
echo "‚ö†Ô∏è SAFETY:"
echo "   ‚Ä¢ Defaults to dry run mode for safety"
echo "   ‚Ä¢ Weekly automated cleanup on Sundays"
echo "   ‚Ä¢ Comprehensive validation before deletion"
echo ""
